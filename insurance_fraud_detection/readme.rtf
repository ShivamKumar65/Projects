{\rtf1\ansi\ansicpg1252\cocoartf2757
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fmodern\fcharset0 Courier-Bold;\f1\fmodern\fcharset0 Courier;}
{\colortbl;\red255\green255\blue255;\red245\green245\blue245;\red0\green0\blue0;\red0\green0\blue255;
}
{\*\expandedcolortbl;;\cssrgb\c96863\c96863\c96863;\cssrgb\c0\c0\c0;\cssrgb\c0\c0\c100000;
}
\paperw11900\paperh16840\margl1440\margr1440\vieww28600\viewh18000\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\b\fs28 \cf0 \cb2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec3 **Challenge:**
\f1\b0  You work for Travelers Insurance Company's fraud detection department as a modeler. Your colleagues, who are not familiar with statistics, would like you to create a predictive model based on historical claim data. Your team is concerned about the fraud detection accuracy as well as the key drivers that cause fraudulence.\cb1 \
\
\pard\pardeftab720\partightenfactor0
\cf0 \cb2 For this case competition, your group is tasked with identifying first-party physical damage fraudulence and explaining the indicators of fraudulent claims.\cb1 \
\pard\pardeftab720\partightenfactor0
\cf4 \cb2 \strokec4   * \cf0 \strokec3 Make sure you use the 'weighted' F1 score - you can just print this using the classification report (bottom right corner).\cb1 \
\cf4 \cb2 \strokec4     * \cf0 \strokec3 https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\cb1 \
\pard\pardeftab720\partightenfactor0

\f0\b \cf0 \cb2 **Challenge:**
\f1\b0  You work for Travelers Insurance Company's fraud detection department as a modeler. Your colleagues, who are not familiar with statistics, would like you to create a predictive model based on historical claim data. Your team is concerned about the fraud detection accuracy as well as the key drivers that cause fraudulence.\cb1 \
\
\pard\pardeftab720\partightenfactor0
\cf0 \cb2 For this case competition, your group is tasked with identifying first-party physical damage fraudulence and explaining the indicators of fraudulent claims.\cb1 \
\pard\pardeftab720\partightenfactor0
\cf4 \cb2 \strokec4   * \cf0 \strokec3 Make sure you use the 'weighted' F1 score - you can just print this using the classification report (bottom right corner).\cb1 \
\cf4 \cb2 \strokec4     * \cf0 \strokec3 https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\cb1 \
\
\
\pard\pardeftab720\partightenfactor0

\f0\b\fs36 \cf0 Interpretability
\f1\b0\fs28 \
\
\
\pard\pardeftab720\partightenfactor0

\f0\b \cf0 \cb2 **Permutation Importance and Predictor Variables:**
\f1\b0 \cb1 \
\
\pard\pardeftab720\partightenfactor0
\cf0 \cb2 The permutation importance analysis highlights the top five predictor variables contributing significantly to the model: 1. past_num_of_claims, 2. annual_income, 3. claim_est_payout, 4. witness_present_ind, and 5. address_change_ind. The permutation plot reveals the varying impact of these variables, with "past_num_of_claims" displaying changes between 0.002 to 0.008, emphasizing its crucial role. The other four variables exhibit changes between 0 and 0.006, with a slightly differing range, but all values falling within this bracket.\cb1 \
\
\pard\pardeftab720\partightenfactor0

\f0\b \cf0 \cb2 **XGBoost Model Performance:**
\f1\b0 \cb1 \
\
\pard\pardeftab720\partightenfactor0
\cf0 \cb2 The XGBoost Model, developed without sampling, emerges as the best-performing model. In the training partition, it achieves an accuracy of 96% for category 0 and 66% for category 1. Transitioning to the test partition, the model maintains strong performance with an accuracy of 91% for category 0 and 11% for category 1. The overall weighted accuracy for f1 in the test partition is commendable at 0.79.\cb1 \
\
\pard\pardeftab720\partightenfactor0

\f0\b \cf0 \cb2 **Confusion Matrix Results:**
\f1\b0 \cb1 \
\
\pard\pardeftab720\partightenfactor0
\cf0 \cb2 Examining the confusion matrices, the training partition reveals 12,842 True Negatives and 1,174 True Positives. In the test partition, the confusion matrix displays 3,139 True Negatives and 381 True Positives, providing a granular understanding of the model's predictive accuracy.\cb1 \
\
\pard\pardeftab720\partightenfactor0

\f0\b \cf0 \cb2 **ICE Plots and Recommendations:**
\f1\b0 \cb1 \
\
\pard\pardeftab720\partightenfactor0
\cf0 \cb2  ICE plots for the five important predictor variables offer insights into their significance, even though some variables closely align with the constant line. The first recommendation emphasizes scrutinizing "past_num_of_claims," as individuals with a higher count are more likely to engage in fraudulent activities, as indicated by the ICE plot. The second recommendation advises a\cb1 \
\cb2  thorough examination of claims with low  "claim_est_payout" values, particularly those below $1800, whicg demonstrate a higher susceptibility to fraud.Lastly, the third recommendation suggests careful scrutiny of claims falling within the mid-level "annual_income" range between 33,000 to 39,000 as they exhibit a higher likelihood of fraud compared to claims outside this range.\cb1 \
\
\
\
\
\pard\pardeftab720\partightenfactor0

\f0\b \cf0 \cb2 **Importance of Predictor Variables:**
\f1\b0 \cb1 \
\
\
\pard\pardeftab720\partightenfactor0
\cf0 \cb2 In conclusion, the significance of the five predictor variables\'97past_num_of_claims, annual_income, claim_est_payout, witness_present_ind, and address_change_ind\'97is underscored. These variables should be accorded heightened importance during the claim verification process, forming a crucial component of an effective and informed assessment strategy.\cb1 \
\
\
\pard\pardeftab720\partightenfactor0

\f0\b\fs36 \cf3 \cb2 \strokec4 Conclusion:
\f1\b0\fs28 \cf4 \
\
\cf0 \cb1 \strokec3 \
\cf4 \cb2 \strokec4 1. \cf0 \strokec3 The project provided valuable insights into efficiently handling substantial datasets using Python, demonstrating adeptness in data cleaning through techniques such as dropping and imputing. This proficiency was particularly beneficial for managing large volumes of data seamlessly.\cb1 \
\
\cf4 \cb2 \strokec4 2. \cf0 \strokec3 The exploration of data visualization tools like seaborn and matplotlib enhanced the ability to discern relationships and patterns within the dataset. Techniques such as utilizing boxplots facilitated the identification of outliers, enabling their systematic cleanup and imputation through streamlined code execution.\cb1 \
\
\cf4 \cb2 \strokec4 3. \cf0 \strokec3 Through the project, a diverse range of models was explored and implemented on a single dataset, each yielding distinct results and accuracies. This experience included understanding and evaluating crucial model parameters, contributing to the selection of the final model.\cb1 \
\
\cf4 \cb2 \strokec4 4. \cf0 \strokec3 The mastery of grid search and hyperparameter tuning added a powerful dimension to model optimization. The efficiency of these techniques in executing complex tasks within a shorter timeframe was evident, making them valuable tools for future endeavors on platforms like Anaconda and Ellipse.\cb1 \
\
\cf4 \cb2 \strokec4 5. \cf0 \strokec3 The project equipped us with the skills to identify significant contributors within a dataset, emphasizing the importance of selecting and plotting key predictor variables. This understanding enhances the ability to focus on influential factors, contributing to more informed decision-making.\cb1 \
\
\cf4 \cb2 \strokec4 6. \cf0 \strokec3 Overall, the project instilled confidence in handling extensive and uncleaned datasets, fostering a robust capability to navigate and analyze large-scale data with proficiency. This newfound confidence extends to working with diverse datasets, laying a solid foundation for future data-driven endeavors.\cb1 \
\
\
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf0 \
\
}